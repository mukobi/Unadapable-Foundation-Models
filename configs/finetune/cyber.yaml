# Description: Example configuration file for finetuning a model with the Hugging Face Trainer
# To demonstrate the necessary fields for the configuration file
# For reference only
defaults:
  - _self_

dataset: cyber
# Supported identifiers are:
#             - "cyber" for the 'cais/wmdp-corpora' dataset with the 'cyber-forget-corpus' subset.
#             - "harmfulqa" for the 'declare-lab/HarmfulQA' dataset.
#             - "toxic" for the 'allenai/real-toxicity-prompts' dataset.
#             - "pile" for the 'NeelNanda/pile-10k' dataset.
# output_dir: "./cyber_fine_tune_outputs"
training_task: "supervised-fine-tuning"

# See https://huggingface.co/transformers/v3.0.2/main_classes/trainer.html#trainingarguments
training_args:
  report_to: "wandb"
  num_train_epochs: 100
  learning_rate: 0.1
  per_device_train_batch_size: 100
  per_device_eval_batch_size: 100
  warmup_steps: 0
  gradient_accumulation_steps: 1  # Update gradients over n batches
  # Training logging
  logging_strategy: "epoch"
  # Evaluations
  evaluation_strategy: "epoch"

  # For saving checkpoints
  # These checkpoints will be saved to the wandb run folder, which is synced to the cloud after the run
  # This is slow and uploads at least hundreds of MBs of data for small runs
  save_strategy: "no"  # Set to 'steps' or 'epochs' to enable
  #save_steps: 100  # Must be multiple of eval_steps
  #save_total_limit: 8
  #load_best_model_at_end: true
  #metric_for_best_model: "eval_loss"
  #greater_is_better: false
