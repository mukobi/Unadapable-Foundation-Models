{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Pruning Toy Example\n",
    "We demonstrate making a simple MLP network pre-trained on MNIST more unadaptable to fine-tuning on Fashion-MNIST by using pruning.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "# from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm.notebook import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "epochs = 10 # TODO rename, also pretrain -> pt below\n",
    "ft_epochs = 1\n",
    "lr = 0.01\n",
    "gamma = 0.7\n",
    "prune_percentage = 0.8\n",
    "seed = 1\n",
    "\n",
    "# Check for CUDA\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders for MNIST and Fashion-MNIST\n",
    "mnist_train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"./data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]  # MNIST stats\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "mnist_test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"./data\",\n",
    "        train=False,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]  # MNIST stats\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=test_batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "fashion_train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        \"./data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.2860,), (0.3530,))]  # Fashion-MNIST stats\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "fashion_test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        \"./data\",\n",
    "        train=False,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.2860,), (0.3530,))]  # Fashion-MNIST stats\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=test_batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Define the 2-layer MLP model\n",
    "class MLPNet(nn.Module):\n",
    "    def __init__(self, hidden_layer_dims=[128]):\n",
    "        super(MLPNet, self).__init__()\n",
    "        # Dynamically create layers based on hidden_layer_dims\n",
    "        prev_dim = 784\n",
    "        fc_layers = []\n",
    "        for dim in hidden_layer_dims:\n",
    "            fc_layers.append(nn.Linear(prev_dim, dim))\n",
    "            fc_layers.append(nn.ReLU())\n",
    "            prev_dim = dim\n",
    "        fc_layers.append(nn.Linear(prev_dim, 10))\n",
    "        self.fc_layers = nn.Sequential(*fc_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.fc_layers(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model_inital = MLPNet().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, device, train_loader, num_epochs=epochs):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    # scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "    model.train()\n",
    "    num_total_batches = len(train_loader) * num_epochs\n",
    "    progress_bar = tqdm(total=num_total_batches, position=0, leave=True)\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            progress_bar.update(1)\n",
    "        if epoch % 2 == 0 or epoch == num_epochs:\n",
    "            print(\n",
    "                f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}\"\n",
    "                f\" ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\"\n",
    "            )\n",
    "    progress_bar.close()\n",
    "\n",
    "\n",
    "# Testing function\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            # sum up batch loss\n",
    "            test_loss += F.nll_loss(output, target, reduction=\"sum\").item()\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)}\"\n",
    "        f\" ({100. * correct / len(test_loader.dataset):.0f}%)\\n\"\n",
    "    )\n",
    "\n",
    "    return test_loss, correct / len(test_loader.dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model on MNIST\n",
    "model_base_pretrain = copy.deepcopy(model_inital)\n",
    "train(model_base_pretrain, device, mnist_train_loader)\n",
    "# scheduler.step()\n",
    "\n",
    "# Evaluate the model on MNIST test set\n",
    "mnist_loss, mnist_accuracy = test(model_base_pretrain, device, mnist_test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune the model to create unadaptable version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning the model...\n",
      "Evaluating the pruned model on MNIST...\n",
      "\n",
      "Test set: Average loss: 0.8657, Accuracy: 8754/10000 (88%)\n",
      "\n",
      "model_base_pt accuracy: 0.9569, model_pruned_pt accuracy: 0.8754\n",
      "model_base_pt loss: 0.1959, model_pruned_pt loss: 0.8657\n"
     ]
    }
   ],
   "source": [
    "# Pruning function\n",
    "def apply_pruning_to_model(model):\n",
    "    parameters_to_prune = (\n",
    "        (model.fc_layers[0], \"weight\"),\n",
    "        (model.fc_layers[2], \"weight\"),\n",
    "        # (model.fc3, \"weight\"),\n",
    "        # Tuple comprehension\n",
    "        # (module, name)\n",
    "        # for name, module in model.named_modules()\n",
    "        # if isinstance(module, nn.Linear)\n",
    "    )\n",
    "\n",
    "    for module, name in parameters_to_prune:\n",
    "        prune.l1_unstructured(module, name, amount=prune_percentage)\n",
    "\n",
    "print(\"Pruning the model...\")\n",
    "model_pruned_pt = copy.deepcopy(model_base_pretrain)\n",
    "apply_pruning_to_model(model_pruned_pt)\n",
    "\n",
    "# Evaluate on MNIST test set\n",
    "print(\"Evaluating the pruned model on MNIST...\")\n",
    "mnist_loss_pruned, mnist_accuracy_pruned = test(model_pruned_pt, device, mnist_test_loader)\n",
    "print(f\"model_base_pt accuracy: {mnist_accuracy:.4f}, model_pruned_pt accuracy: {mnist_accuracy_pruned:.4f}\")\n",
    "print(f\"model_base_pt loss: {mnist_loss:.4f}, model_pruned_pt loss: {mnist_loss_pruned:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune both the original and the pruned models on Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the pre-trained model on Fashion-MNIST\n",
      "\n",
      "Test set: Average loss: 11.5359, Accuracy: 537/10000 (5%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 19.2064, Accuracy: 1030/10000 (10%)\n",
      "\n",
      "model_base_pt accuracy: 0.0537, model_pruned_pt accuracy: 0.1030\n",
      "model_base_pt loss: 11.5359, model_pruned_pt loss: 19.2064\n",
      "Training the base model on Fashion-MNIST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:14<00:00, 65.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [29984/60000 (100%)]\tLoss: 0.607832\n",
      "Training the pruned model on Fashion-MNIST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:14<00:00, 64.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [29984/60000 (100%)]\tLoss: 0.983501\n",
      "\n",
      "Test set: Average loss: 0.5130, Accuracy: 8206/10000 (82%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.5591, Accuracy: 8060/10000 (81%)\n",
      "\n",
      "Evaluating the fine-tuned models on Fashion-MNIST\n",
      "model_base_ft accuracy: 0.8206, model_pruned_ft accuracy: 0.8060\n",
      "model_base_ft loss: 0.5130, model_pruned_ft loss: 0.5591\n"
     ]
    }
   ],
   "source": [
    "model_base_ft = copy.deepcopy(model_base_pretrain)\n",
    "model_pruned_ft = copy.deepcopy(model_pruned_pt)\n",
    "\n",
    "# Evaluate the pre-trained model on Fashion-MNIST\n",
    "print(\"Evaluating the pre-trained model on Fashion-MNIST\")\n",
    "ft_loss_base_pretrain, ft_acc_base_pretrain = test(model_base_pretrain, device, fashion_test_loader)\n",
    "ft_loss_pruned_pretrain, ft_acc_pruned_pretrain = test(model_pruned_pt, device, fashion_test_loader)\n",
    "print(f\"model_base_pt accuracy: {ft_acc_base_pretrain:.4f}, model_pruned_pt accuracy: {ft_acc_pruned_pretrain:.4f}\")\n",
    "print(f\"model_base_pt loss: {ft_loss_base_pretrain:.4f}, model_pruned_pt loss: {ft_loss_pruned_pretrain:.4f}\")\n",
    "\n",
    "# Train the models on Fashion-MNIST\n",
    "print(\"Training the base model on Fashion-MNIST\")\n",
    "train(model_base_ft, device, fashion_train_loader, num_epochs=ft_epochs)\n",
    "\n",
    "print(\"Training the pruned model on Fashion-MNIST\")\n",
    "train(model_pruned_ft, device, fashion_train_loader, num_epochs=ft_epochs)\n",
    "\n",
    "# Calculate metrics\n",
    "ft_loss_base_ft, ft_acc_base_ft = test(model_base_ft, device, fashion_test_loader)\n",
    "ft_loss_pruned_ft, ft_acc_pruned_ft = test(model_pruned_ft, device, fashion_test_loader)\n",
    "\n",
    "print(\"Evaluating the fine-tuned models on Fashion-MNIST\")\n",
    "print(f\"model_base_ft accuracy: {ft_acc_base_ft:.4f}, model_pruned_ft accuracy: {ft_acc_pruned_ft:.4f}\")\n",
    "print(f\"model_base_ft loss: {ft_loss_base_ft:.4f}, model_pruned_ft loss: {ft_loss_pruned_ft:.4f}\")\n",
    "\n",
    "# loss_gap_ratio = abs(fine_tune_accuracy_base - fine_tune_accuracy_pruned)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and save the results\n",
    "print(f\"MNIST Test Accuracy before fine-tuning: {mnist_accuracy:.4f}\")\n",
    "print(\n",
    "    f\"Fashion-MNIST Test Accuracy after fine-tuning (Base Model): {fine_tune_accuracy_base:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Fashion-MNIST Test Accuracy after fine-tuning (Pruned Model): {fine_tune_accuracy_pruned:.4f}\"\n",
    ")\n",
    "print(f\"Loss Gap Ratio: {loss_gap_ratio:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ais",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
